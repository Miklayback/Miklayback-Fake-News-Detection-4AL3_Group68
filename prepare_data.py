import os
import re
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
import joblib

# ========== 0. è‡ªåŠ¨åˆ‡æ¢åˆ°è„šæœ¬æ‰€åœ¨ç›®å½• ==========
os.chdir(os.path.dirname(__file__))
print("Current working directory:", os.getcwd())

# ========== 1. è¯»å–æ•°æ® ==========
cols = [
    "json_id", "label", "statement", "subject", "speaker", "speaker_job_title",
    "state_info", "party_affiliation", "barely_true_counts", "false_counts",
    "half_true_counts", "mostly_true_counts", "pants_on_fire_counts", "context"
]

train = pd.read_csv("train.tsv", sep="\t", header=None, names=cols)
valid = pd.read_csv("valid.tsv", sep="\t", header=None, names=cols)
test  = pd.read_csv("test.tsv",  sep="\t", header=None, names=cols)

print("Train:", train.shape)
print("Valid:", valid.shape)
print("Test :", test.shape)
print(train["label"].value_counts())

# ========== 2. ç¼ºå¤±å€¼ä¸é‡å¤å€¼å¤„ç† ==========
train = train.dropna(subset=["statement", "label"])
train = train.drop_duplicates(subset=["statement", "speaker", "context"])

print("\nMissing value ratio (top 5):")
print(train.isnull().mean().sort_values(ascending=False).head())

# ========== 3. æ–‡æœ¬æ¸…æ´—å‡½æ•° ==========
# import nltk
# nltk.download('wordnet')
# nltk.download('omw-1.4')
# from nltk.stem import WordNetLemmatizer
# lemm = WordNetLemmatizer()

def clean_text(text):
    text = text.lower()
    text = re.sub(r"[^a-z0-9%?!$ ]", " ", text)
    # ä¸´æ—¶å…³é—­è¯å½¢è¿˜åŸä»¥é¿å… NLTK ä¸‹è½½å¡æ­»
    # text = ' '.join(lemm.lemmatize(w) for w in text.split())
    return text.strip()

train["clean_statement"] = train["statement"].apply(clean_text)
valid["clean_statement"] = valid["statement"].apply(clean_text)
test["clean_statement"]  = test["statement"].apply(clean_text)

# ========== 4. å¯è§†åŒ–ï¼šè¯­å¥é•¿åº¦åˆ†å¸ƒ & æ ‡ç­¾åˆ†å¸ƒ ==========
train["len"] = train["clean_statement"].apply(lambda x: len(x.split()))

plt.figure(figsize=(6,4))
sns.histplot(train["len"], bins=30, color="steelblue")
plt.title("Distribution of Statement Lengths")
plt.xlabel("Number of tokens")
plt.ylabel("Count")
plt.tight_layout()
plt.savefig("len_dist.png")
plt.close()

plt.figure(figsize=(6,4))
sns.countplot(y=train["label"], order=train["label"].value_counts().index, palette="viridis")
plt.title("Label Distribution in Training Set")
plt.xlabel("Count")
plt.ylabel("Label")
plt.tight_layout()
plt.savefig("label_dist.png")
plt.close()

print("\nâœ… Saved visualization: len_dist.png, label_dist.png")

# ========== 5. TF-IDF ç‰¹å¾æå– ==========
tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1,2))
X_train = tfidf.fit_transform(train["clean_statement"])
X_valid = tfidf.transform(valid["clean_statement"])
X_test  = tfidf.transform(test["clean_statement"])

print("TF-IDF matrix shape:", X_train.shape)
joblib.dump(tfidf, "tfidf_vectorizer.pkl")

# ========== 6. ä¿å­˜æ¸…æ´—åçš„æ•°æ® ==========
train.to_csv("train_clean.csv", index=False)
valid.to_csv("valid_clean.csv", index=False)
test.to_csv("test_clean.csv", index=False)

print("\nâœ… Saved cleaned datasets:")
print("   train_clean.csv, valid_clean.csv, test_clean.csv")
print("âœ… Saved TF-IDF vectorizer: tfidf_vectorizer.pkl")

print("\nğŸ¯ All preprocessing steps completed successfully!")
print("   Clean data ready for model ğŸš€")